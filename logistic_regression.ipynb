{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-12T00:02:48.200020Z",
     "start_time": "2025-01-12T00:02:48.194749Z"
    }
   },
   "source": [
    "from enum import Enum\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import resample"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T00:02:48.226690Z",
     "start_time": "2025-01-12T00:02:48.223316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Regularization(Enum):\n",
    "    NONE = 0\n",
    "    L1 = 1\n",
    "    L2 = 2\n",
    "    ELASTIC_NET = 3"
   ],
   "id": "2df0b73daffecad8",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T00:02:48.249878Z",
     "start_time": "2025-01-12T00:02:48.237404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LOGISTIC REGRESSION\n",
    "\n",
    "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
    "    out = 1 / (1 + np.exp(-x))\n",
    "    return out\n",
    "\n",
    "\n",
    "def predict(X: np.ndarray, weights: np.ndarray) -> np.ndarray:\n",
    "    return sigmoid(X @ weights)\n",
    "\n",
    "\n",
    "def compute_cost(X: np.ndarray,\n",
    "                 y: np.ndarray,\n",
    "                 w: np.ndarray,\n",
    "                 regularization: Regularization = Regularization.NONE,\n",
    "                 lambda_: float = 0.01,\n",
    "                 elastic_ratio: float = 0.5) -> np.ndarray:\n",
    "    preds = predict(X, w)\n",
    "    preds = np.clip(preds, 1e-10, 1 - 1e-10)\n",
    "    cost = - np.sum(y * np.log(preds) + (1 - y) * np.log(1 - preds))\n",
    "\n",
    "    if regularization == Regularization.L1:\n",
    "        cost += lambda_ * np.sum(np.abs(w))\n",
    "    elif regularization == Regularization.L2:\n",
    "        cost += lambda_ * np.sum(w ** 2) / 2\n",
    "    elif regularization == Regularization.ELASTIC_NET:\n",
    "        l1_term = elastic_ratio * np.sum(np.abs(w))\n",
    "        l2_term = (1 - elastic_ratio) * np.sum(w ** 2) / 2\n",
    "        cost += lambda_ * (l1_term + l2_term)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def compute_gradient(X: np.ndarray,\n",
    "                     y: np.ndarray,\n",
    "                     w: np.ndarray,\n",
    "                     regularization: Regularization = Regularization.NONE,\n",
    "                     lambda_=0.01,\n",
    "                     elastic_ratio=0.5) -> np.ndarray:\n",
    "    grad = X.T @ (sigmoid(X @ w) - y)\n",
    "\n",
    "    if regularization == Regularization.L1:\n",
    "        grad += lambda_ * np.sign(w)\n",
    "    elif regularization == Regularization.L2:\n",
    "        grad += lambda_ * w\n",
    "    elif regularization == Regularization.ELASTIC_NET:\n",
    "        l1_grad = elastic_ratio * np.sign(w)\n",
    "        l2_grad = (1 - elastic_ratio) * w\n",
    "        grad += lambda_ * (l1_grad + l2_grad)\n",
    "\n",
    "    return grad\n",
    "\n",
    "\n",
    "def compute_confusion_matrix(X: np.ndarray, y: np.ndarray, w: np.ndarray, threshold=0.5) -> tuple[int, int, int, int]:\n",
    "    predictions = predict(X, w)\n",
    "    binary_predictions = (predictions >= threshold).astype(int)\n",
    "\n",
    "    true_positives = np.sum((binary_predictions == 1) & (y == 1))\n",
    "    false_positives = np.sum((binary_predictions == 1) & (y == 0))\n",
    "    false_negatives = np.sum((binary_predictions == 0) & (y == 1))\n",
    "    true_negatives = np.sum((binary_predictions == 0) & (y == 0))\n",
    "\n",
    "    return true_positives, false_positives, false_negatives, true_negatives\n",
    "\n",
    "\n",
    "def compute_accuracy(X: np.ndarray, y: np.ndarray, w: np.ndarray, threshold=0.5) -> float:\n",
    "    predictions = predict(X, w)\n",
    "    binary_predictions = predictions >= threshold\n",
    "    correct_predictions = np.sum(binary_predictions == y)\n",
    "\n",
    "    acc = correct_predictions / y.shape[0]\n",
    "\n",
    "    return acc\n",
    "\n",
    "\n",
    "def compute_precision(X, y, w):\n",
    "    true_positives, false_positives, false_negatives, true_negatives = compute_confusion_matrix(X, y, w)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "\n",
    "    return precision\n",
    "\n",
    "\n",
    "def compute_recall(X, y, w):\n",
    "    true_positives, false_positives, false_negatives, true_negatives = compute_confusion_matrix(X, y, w)\n",
    "\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    return recall\n",
    "\n",
    "\n",
    "def compute_f1_score(X, y, w):\n",
    "    precision = compute_precision(X, y, w)\n",
    "    recall = compute_recall(X, y, w)\n",
    "\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    return f1_score\n",
    "\n",
    "\n",
    "def cross_validate_logistic_regression(X: np.ndarray,\n",
    "                                       y: np.ndarray,\n",
    "                                       lambdas: list[float],\n",
    "                                       alpha: float,\n",
    "                                       no_iterations: int,\n",
    "                                       regularization: Regularization = Regularization.NONE,\n",
    "                                       elastic_ratio=0.5,\n",
    "                                       k=5) -> float:\n",
    "    best_lambda = lambdas[0]\n",
    "    best_cost = float('inf')\n",
    "\n",
    "    for lambda_ in lambdas:\n",
    "        kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        avg_cost = 0\n",
    "\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            w = np.random.randn(X_train.shape[1])\n",
    "            w, _, _, _, _, _ = train(X_train, y_train, w, alpha, no_iterations, regularization, lambda_, elastic_ratio)\n",
    "            cost = compute_cost(X_val, y_val, w, regularization, lambda_, elastic_ratio)\n",
    "            avg_cost += cost\n",
    "\n",
    "        avg_cost /= k\n",
    "        if avg_cost < best_cost:\n",
    "            best_cost = avg_cost\n",
    "            best_lambda = lambda_\n",
    "\n",
    "    return best_lambda\n",
    "\n",
    "\n",
    "def train(X: np.ndarray,\n",
    "          y: np.ndarray,\n",
    "          w: np.ndarray,\n",
    "          alpha: float,\n",
    "          no_iterations: int,\n",
    "          regularization: Regularization = Regularization.NONE,\n",
    "          lambda_=0.01,\n",
    "          elastic_ratio=0.5\n",
    "          ) -> tuple[\n",
    "    np.ndarray, list[float], list[float], list[float], list[float], list[float]]:\n",
    "    costs = []\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "\n",
    "    for step in range(no_iterations):\n",
    "        grad = compute_gradient(X, y, w, regularization, lambda_, elastic_ratio)\n",
    "        w = w - alpha * grad\n",
    "\n",
    "        costs.append(compute_cost(X, y, w, regularization, lambda_, elastic_ratio))\n",
    "        accuracies.append(compute_accuracy(X, y, w))\n",
    "        precisions.append(compute_precision(X, y, w))\n",
    "        recalls.append(compute_recall(X, y, w))\n",
    "        f1_scores.append(compute_f1_score(X, y, w))\n",
    "\n",
    "    return w, costs, accuracies, precisions, recalls, f1_scores\n",
    "\n",
    "def bagging_logistic_regression(X_train: np.ndarray,\n",
    "                                y_train: np.ndarray,\n",
    "                                X_test: np.ndarray,\n",
    "                                y_test: np.ndarray,\n",
    "                                n_estimators: int,\n",
    "                                alpha: float,\n",
    "                                no_iterations: int,\n",
    "                                regularization: Regularization = Regularization.NONE,\n",
    "                                lambda_: float = 0.01,\n",
    "                                elastic_ratio: float = 0.5) -> np.ndarray:\n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(n_estimators):\n",
    "        X_resampled, y_resampled = resample(X_train, y_train, replace=True)\n",
    "        \n",
    "        w = np.zeros(X_resampled.shape[1])\n",
    "        w, _, _, _, _, _ = train(X_resampled, y_resampled, w, alpha, no_iterations, regularization, lambda_, elastic_ratio)\n",
    "        \n",
    "        preds = predict(X_test, w)\n",
    "        predictions.append(preds)\n",
    "        \n",
    "    averaged_predictions = np.mean(predictions, axis=0)\n",
    "    final_predictions = ((averaged_predictions >= 0.5).astype(int) == y_test)\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "def test(X: np.ndarray, y: np.ndarray, w: np.ndarray) -> tuple[float, float, float, float, float]:\n",
    "    cost = compute_cost(X, y, w)\n",
    "    accuracy = compute_accuracy(X, y, w)\n",
    "    precision = compute_precision(X, y, w)\n",
    "    recall = compute_recall(X, y, w)\n",
    "    f1_score = compute_f1_score(X, y, w)\n",
    "\n",
    "    return cost, accuracy, precision, recall, f1_score"
   ],
   "id": "338c0733eecf10db",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T00:02:48.273470Z",
     "start_time": "2025-01-12T00:02:48.269208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TRAIN AND TEST ALGORITHM\n",
    "\n",
    "def evaluate_algorithm(X_train,\n",
    "                       y_train,\n",
    "                       X_test,\n",
    "                       y_test,\n",
    "                       w,\n",
    "                       alpha,\n",
    "                       no_iterations,\n",
    "                       regularization: Regularization = Regularization.NONE,\n",
    "                       lambda_=0.01,\n",
    "                       elastic_ratio=0.5\n",
    "                       ):\n",
    "    w, costs_train, accuracies_train, precisions_train, recalls_train, f1_scores_train = train(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        w,\n",
    "        alpha,\n",
    "        no_iterations,\n",
    "        regularization,\n",
    "        lambda_,\n",
    "        elastic_ratio\n",
    "    )\n",
    "\n",
    "    cost_test, accuracy_test, precision_test, recall_test, f1_score_test = test(X_test, y_test, w)\n",
    "\n",
    "    print(f'Stochastic Gradient Descent average cost on test: {cost_test / X_test.shape[0]}')\n",
    "    print(f'Stochastic Gradient Descent accuracy on test: {accuracy_test}')\n",
    "    print(f'Stochastic Gradient Descent precision on test: {precision_test}')\n",
    "    print(f'Stochastic Gradient Descent recall on test: {recall_test}')\n",
    "    print(f'Stochastic Gradient Descent F1 Score on test: {f1_score_test}')\n",
    "\n",
    "    #display the cost of the logistic regression on the training set\n",
    "    plt.figure()\n",
    "    plt.plot(costs_train)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.title('Stochastic Gradient Descent')\n",
    "    plt.show()\n",
    "\n",
    "    #display the accuracy of the logistic regression on the training set\n",
    "    plt.figure()\n",
    "    plt.plot(accuracies_train)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Stochastic Gradient Descent')\n",
    "    plt.show()\n",
    "\n",
    "    #display the precision of the logistic regression on the training set\n",
    "    plt.figure()\n",
    "    plt.plot(precisions_train)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Stochastic Gradient Descent')\n",
    "    plt.show()\n",
    "\n",
    "    #display the recall of the logistic regression on the training set\n",
    "    plt.figure()\n",
    "    plt.plot(recalls_train)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.title('Stochastic Gradient Descent')\n",
    "    plt.show()\n",
    "\n",
    "    #display the f1_score of the logistic regression on the training set\n",
    "    plt.figure()\n",
    "    plt.plot(f1_scores_train)\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title('Stochastic Gradient Descent')\n",
    "    plt.show()"
   ],
   "id": "dd4ca3682a9f2c0d",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-12T00:02:48.293487Z",
     "start_time": "2025-01-12T00:02:48.292068Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "938cc39f3c62965c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
